{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Drive service authorized successfully.\n",
      "Date range for filtering: 20250309 to 20250314\n",
      "Total files found in the folder: 4142\n",
      "Number of files matching the date range: 34\n",
      "                      name  size_mb\n",
      "0         20250313_CHN.csv    13.15\n",
      "1         20250313_KOL.csv     4.45\n",
      "2         20250313_NCR.csv    38.53\n",
      "3      20250313_Mumbai.csv    21.59\n",
      "4         20250313_HYD.csv    16.48\n",
      "5        20250313_Pune.csv     6.16\n",
      "6   20250313_Bengaluru.csv    27.81\n",
      "7         20250312_CHN.csv    13.09\n",
      "8         20250312_NCR.csv    39.70\n",
      "9         20250312_HYD.csv    15.42\n",
      "10        20250312_KOL.csv     5.91\n",
      "11       20250312_Pune.csv     6.24\n",
      "12  20250312_Bengaluru.csv    28.10\n",
      "13        20250311_HYD.csv    14.94\n",
      "14        20250311_CHN.csv    11.48\n",
      "15        20250311_KOL.csv     4.76\n",
      "16        20250311_NCR.csv    41.31\n",
      "17  20250311_Bengaluru.csv    26.16\n",
      "18     20250311_Mumbai.csv    18.48\n",
      "19       20250311_Pune.csv     5.33\n",
      "20        20250310_CHN.csv    11.66\n",
      "21        20250310_KOL.csv     4.93\n",
      "22        20250310_HYD.csv    14.97\n",
      "23        20250310_NCR.csv    43.36\n",
      "24     20250310_Mumbai.csv    19.43\n",
      "25        20250309_NCR.csv    40.33\n",
      "26  20250310_Bengaluru.csv    25.01\n",
      "27       20250310_Pune.csv     7.32\n",
      "28        20250309_HYD.csv    14.55\n",
      "29        20250309_CHN.csv    12.31\n",
      "30        20250309_KOL.csv     4.51\n",
      "31     20250309_Mumbai.csv    20.42\n",
      "32       20250309_Pune.csv     6.97\n",
      "33  20250309_Bengaluru.csv    23.51 \n",
      "\n",
      "Total Data to be processed 608.37 MB\n",
      "\n",
      "File number 1 - Downloading file: 20250313_CHN.csv (Resuming from 0 bytes)\n",
      "Download 76% complete for 20250313_CHN.csv.\n",
      "Download 100% complete for 20250313_CHN.csv.\n",
      "\n",
      "File number 2 - Downloading file: 20250313_KOL.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250313_KOL.csv.\n",
      "\n",
      "File number 3 - Downloading file: 20250313_NCR.csv (Resuming from 0 bytes)\n",
      "Download 25% complete for 20250313_NCR.csv.\n",
      "Download 51% complete for 20250313_NCR.csv.\n",
      "Download 77% complete for 20250313_NCR.csv.\n",
      "Download 100% complete for 20250313_NCR.csv.\n",
      "\n",
      "File number 4 - Downloading file: 20250313_Mumbai.csv (Resuming from 0 bytes)\n",
      "Download 46% complete for 20250313_Mumbai.csv.\n",
      "Download 92% complete for 20250313_Mumbai.csv.\n",
      "Download 100% complete for 20250313_Mumbai.csv.\n",
      "\n",
      "File number 5 - Downloading file: 20250313_HYD.csv (Resuming from 0 bytes)\n",
      "Download 60% complete for 20250313_HYD.csv.\n",
      "Download 100% complete for 20250313_HYD.csv.\n",
      "\n",
      "File number 6 - Downloading file: 20250313_Pune.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250313_Pune.csv.\n",
      "\n",
      "File number 7 - Downloading file: 20250313_Bengaluru.csv (Resuming from 0 bytes)\n",
      "Download 35% complete for 20250313_Bengaluru.csv.\n",
      "Download 71% complete for 20250313_Bengaluru.csv.\n",
      "Download 100% complete for 20250313_Bengaluru.csv.\n",
      "\n",
      "File number 8 - Downloading file: 20250312_CHN.csv (Resuming from 0 bytes)\n",
      "Download 76% complete for 20250312_CHN.csv.\n",
      "Download 100% complete for 20250312_CHN.csv.\n",
      "\n",
      "File number 9 - Downloading file: 20250312_NCR.csv (Resuming from 0 bytes)\n",
      "Download 25% complete for 20250312_NCR.csv.\n",
      "Download 50% complete for 20250312_NCR.csv.\n",
      "Error processing file 20250312_NCR.csv: The read operation timed out\n",
      "\n",
      "File number 10 - Downloading file: 20250312_HYD.csv (Resuming from 0 bytes)\n",
      "Download 64% complete for 20250312_HYD.csv.\n",
      "Download 100% complete for 20250312_HYD.csv.\n",
      "\n",
      "File number 11 - Downloading file: 20250312_KOL.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250312_KOL.csv.\n",
      "\n",
      "File number 12 - Downloading file: 20250312_Pune.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250312_Pune.csv.\n",
      "\n",
      "File number 13 - Downloading file: 20250312_Bengaluru.csv (Resuming from 0 bytes)\n",
      "Download 35% complete for 20250312_Bengaluru.csv.\n",
      "Download 71% complete for 20250312_Bengaluru.csv.\n",
      "Download 100% complete for 20250312_Bengaluru.csv.\n",
      "\n",
      "File number 14 - Downloading file: 20250311_HYD.csv (Resuming from 0 bytes)\n",
      "Download 66% complete for 20250311_HYD.csv.\n",
      "Download 100% complete for 20250311_HYD.csv.\n",
      "\n",
      "File number 15 - Downloading file: 20250311_CHN.csv (Resuming from 0 bytes)\n",
      "Download 87% complete for 20250311_CHN.csv.\n",
      "Download 100% complete for 20250311_CHN.csv.\n",
      "\n",
      "File number 16 - Downloading file: 20250311_KOL.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250311_KOL.csv.\n",
      "\n",
      "File number 17 - Downloading file: 20250311_NCR.csv (Resuming from 0 bytes)\n",
      "Download 24% complete for 20250311_NCR.csv.\n",
      "Download 48% complete for 20250311_NCR.csv.\n",
      "Download 72% complete for 20250311_NCR.csv.\n",
      "Download 96% complete for 20250311_NCR.csv.\n",
      "Download 100% complete for 20250311_NCR.csv.\n",
      "\n",
      "File number 18 - Downloading file: 20250311_Bengaluru.csv (Resuming from 0 bytes)\n",
      "Download 38% complete for 20250311_Bengaluru.csv.\n",
      "Download 76% complete for 20250311_Bengaluru.csv.\n",
      "Download 100% complete for 20250311_Bengaluru.csv.\n",
      "\n",
      "File number 19 - Downloading file: 20250311_Mumbai.csv (Resuming from 0 bytes)\n",
      "Download 54% complete for 20250311_Mumbai.csv.\n",
      "Download 100% complete for 20250311_Mumbai.csv.\n",
      "\n",
      "File number 20 - Downloading file: 20250311_Pune.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250311_Pune.csv.\n",
      "\n",
      "File number 21 - Downloading file: 20250310_CHN.csv (Resuming from 0 bytes)\n",
      "Download 85% complete for 20250310_CHN.csv.\n",
      "Download 100% complete for 20250310_CHN.csv.\n",
      "\n",
      "File number 22 - Downloading file: 20250310_KOL.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250310_KOL.csv.\n",
      "\n",
      "File number 23 - Downloading file: 20250310_HYD.csv (Resuming from 0 bytes)\n",
      "Download 66% complete for 20250310_HYD.csv.\n",
      "Download 100% complete for 20250310_HYD.csv.\n",
      "\n",
      "File number 24 - Downloading file: 20250310_NCR.csv (Resuming from 0 bytes)\n",
      "Download 23% complete for 20250310_NCR.csv.\n",
      "Download 46% complete for 20250310_NCR.csv.\n",
      "Download 69% complete for 20250310_NCR.csv.\n",
      "Download 92% complete for 20250310_NCR.csv.\n",
      "Download 100% complete for 20250310_NCR.csv.\n",
      "\n",
      "File number 25 - Downloading file: 20250310_Mumbai.csv (Resuming from 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import pytz, os,json, requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "from googleapiclient.errors import HttpError\n",
    "import googleapiclient.http\n",
    "\n",
    "# Path to your service account key file and the scopes needed for the Drive API\n",
    "SERVICE_ACCOUNT_FILE = \"/Users/sachin/TheJuniorDataScientist/credentials/sachin_service account.json\"\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Authenticate and build the Drive API client\n",
    "def authenticate_drive_service():\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        drive_service = build('drive', 'v3', credentials=credentials)\n",
    "        print(\"Google Drive service authorized successfully.\")\n",
    "        return drive_service\n",
    "    except Exception as e:\n",
    "        print(\"Error during authentication:\", e)\n",
    "        raise\n",
    "\n",
    "# Generate date range (YYYYMMDD format)\n",
    "def get_date_range(local_tz, start_date=None, end_date=None, days=None):\n",
    "    current_time = datetime.now(pytz.utc).astimezone(local_tz)\n",
    "    if end_date:\n",
    "        end_date = datetime.strptime(end_date, \"%Y%m%d\").astimezone(local_tz)\n",
    "    else:\n",
    "        end_date = current_time\n",
    "\n",
    "    if start_date:\n",
    "        start_date = datetime.strptime(start_date, \"%Y%m%d\").astimezone(local_tz)\n",
    "    elif days:\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "    else:\n",
    "        start_date = end_date - timedelta(days=10)\n",
    "\n",
    "    return start_date.strftime(\"%Y%m%d\"), end_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "def fetch_files_with_dates(drive_service, folder_id, date_range):\n",
    "    try:\n",
    "        query = f\"'{folder_id}' in parents and mimeType='text/csv' and trashed=false\"\n",
    "        \n",
    "        files = []\n",
    "        page_token = None  # Initialize page token\n",
    "        \n",
    "        while True:\n",
    "            # Fetch a page of files\n",
    "            response = drive_service.files().list(\n",
    "                q=query,\n",
    "                fields=\"nextPageToken, files(id, name, size)\",\n",
    "                pageToken=page_token\n",
    "            ).execute()\n",
    "\n",
    "            # Extend the list with new files from this page\n",
    "            files.extend(response.get('files', []))\n",
    "\n",
    "            # Check if there's another page\n",
    "            page_token = response.get('nextPageToken')\n",
    "            if not page_token:\n",
    "                break  # No more pages, exit loop\n",
    "        \n",
    "        print(f\"Total files found in the folder: {len(files)}\")\n",
    "\n",
    "        # Filter files by date in filenames\n",
    "        filtered_files = []\n",
    "        for file in files:\n",
    "            for date in date_range:\n",
    "                if date in file['name']:\n",
    "                    file['size_mb'] = round(int(file.get('size', 0)) / (1024 * 1024), 2)  # Convert size to MB\n",
    "                    filtered_files.append(file)\n",
    "                    break\n",
    "\n",
    "        print(f\"Number of files matching the date range: {len(filtered_files)}\")\n",
    "        return filtered_files\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def process_files(drive_service, files, required_columns, checkpoint_file=\"download_progress.json\", save_interval = 50 * 1024 * 1024, chunksize=10 * 1024 * 1024):\n",
    "    combined_data = []\n",
    "    last_save_time = datetime.now()\n",
    "\n",
    "    try:\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            download_progress = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        download_progress = {}\n",
    "\n",
    "    for i, file in enumerate(files):\n",
    "        try:\n",
    "            file_id = file['id']\n",
    "            file_name = file['name']\n",
    "            file_size = int(file.get('size', 0))\n",
    "            bytes_downloaded = download_progress.get(file_name, 0)\n",
    "\n",
    "            if bytes_downloaded >= file_size:\n",
    "                print(f\"{file_name} has already been downloaded. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nFile number {i+1} - Downloading file: {file_name} (Resuming from {bytes_downloaded} bytes)\")\n",
    "\n",
    "            request = drive_service.files().get_media(fileId=file_id)\n",
    "            file_stream = BytesIO()\n",
    "            downloader = googleapiclient.http.MediaIoBaseDownload(file_stream, request, chunksize=chunksize)\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                try:\n",
    "                    status, done = downloader.next_chunk()\n",
    "                    bytes_downloaded += len(file_stream.getvalue())\n",
    "                    download_progress[file_name] = bytes_downloaded\n",
    "\n",
    "                    if (datetime.now() - last_save_time).total_seconds() > 60 or bytes_downloaded % save_interval < chunksize:\n",
    "                        with open(checkpoint_file, \"w\") as f:\n",
    "                            json.dump(download_progress, f)\n",
    "                        last_save_time = datetime.now()\n",
    "\n",
    "                    print(f\"Download {int(status.progress() * 100)}% complete for {file_name}.\")\n",
    "                except (requests.ConnectionError, HttpError) as e:\n",
    "                    print(f\"Network error during download: {e}. Retrying...\")\n",
    "                    break\n",
    "\n",
    "            file_stream.seek(0)\n",
    "            df = pd.read_csv(file_stream, usecols=required_columns)\n",
    "            df = df[required_columns]\n",
    "            df['source_file'] = file_name\n",
    "            combined_data.append(df)\n",
    "\n",
    "            if file_name in download_progress:\n",
    "                del download_progress[file_name]\n",
    "                with open(checkpoint_file, \"w\") as f:\n",
    "                    json.dump(download_progress, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    if combined_data:\n",
    "        combined_data = [df for df in combined_data if not df.empty]\n",
    "        if combined_data:\n",
    "            final_df = pd.concat(combined_data, ignore_index=True)\n",
    "            final_df[['to_qty']] = final_df[['to_qty']].fillna(0)\n",
    "            print(\"All files processed and merged successfully.\")\n",
    "            return final_df\n",
    "    else:\n",
    "        print(\"No files were successfully processed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Step 1: Authenticate Drive Service\n",
    "drive_service = authenticate_drive_service()\n",
    "\n",
    "local_tz = pytz.timezone(\"Asia/Kolkata\")\n",
    "start_date_input = None # \"20250201\"\n",
    "end_date_input =  None #\"20250225\"\n",
    "days_range = 5\n",
    "\n",
    "start_date, end_date = get_date_range(local_tz, start_date_input, end_date_input, days_range)\n",
    "date_range = pd.date_range(start=start_date, end=end_date).strftime(\"%Y%m%d\").tolist()\n",
    "print(f\"Date range for filtering: {start_date} to {end_date}\")\n",
    "\n",
    "# Step 3: Fetch Files Matching the Date Range\n",
    "folder_id = '1akFe2_iKCqoZ0lAETRipXxDSKYnFmx2x'\n",
    "filtered_files = fetch_files_with_dates(drive_service, folder_id, date_range)\n",
    "file_df = pd.DataFrame(filtered_files)\n",
    "file_df.drop(columns=['size','id'],inplace=True, axis=1)\n",
    "print(file_df, f\"\\n\\nTotal Data to be processed {file_df['size_mb'].sum()} MB\")\n",
    "\n",
    "# Step 4: Process Files and Combine Data\n",
    "required_columns = ['dd', 'mm', 'yyyy', 'mh_code', 'type', 'store_name', 'to_qty','store_id', 'product_variant_id']\n",
    "final_df = process_files(drive_service, filtered_files, required_columns)\n",
    "\n",
    "print('Total rows before removing null values :',final_df.shape[0])\n",
    "### Printing Null Records\n",
    "#final_df[final_df[['dd', 'mm', 'yyyy', 'mh_code', 'type', 'store_name', 'store_id']].isna().all(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dropping Null records\n",
    "main_df = final_df.dropna(subset=['dd', 'mm', 'yyyy', 'mh_code', 'type', 'store_name', 'store_id', 'product_variant_id'], how='all').copy()\n",
    "columns_to_convert = ['dd', 'mm', 'yyyy']\n",
    "try:\n",
    "    main_df[columns_to_convert] = main_df[columns_to_convert].fillna(1).astype(int)\n",
    "    main_df.rename(columns={'yyyy': 'year', 'mm': 'month', 'dd': 'day'}, inplace=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error during  conversion: {e}\")\n",
    "try:\n",
    "    # Creating the date column from dd, mm, yyyy\n",
    "    main_df['date'] = pd.to_datetime(main_df[['year', 'month', 'day']])\n",
    "    print(\"Date column created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during date conversion: {e}\")\n",
    "\n",
    "\n",
    "main_df = main_df.groupby(['type','mh_code', 'store_name', 'store_id', 'source_file', 'date', 'product_variant_id']).agg({'to_qty': 'sum'}).reset_index()\n",
    "main_df.rename(columns={'store_name': 'Destination_store_name', 'store_id': 'Destination_store_id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

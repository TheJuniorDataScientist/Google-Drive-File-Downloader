{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Drive service authorized successfully.\n",
      "Date range for filtering: 20250309 to 20250314\n",
      "Total files found in the folder: 4142\n",
      "Number of files matching the date range: 34\n",
      "                      name  size_mb\n",
      "0         20250313_CHN.csv    13.15\n",
      "1         20250313_KOL.csv     4.45\n",
      "2         20250313_NCR.csv    38.53\n",
      "3      20250313_Mumbai.csv    21.59\n",
      "4         20250313_HYD.csv    16.48\n",
      "5        20250313_Pune.csv     6.16\n",
      "6   20250313_Bengaluru.csv    27.81\n",
      "7         20250312_CHN.csv    13.09\n",
      "8         20250312_NCR.csv    39.70\n",
      "9         20250312_HYD.csv    15.42\n",
      "10        20250312_KOL.csv     5.91\n",
      "11       20250312_Pune.csv     6.24\n",
      "12  20250312_Bengaluru.csv    28.10\n",
      "13        20250311_HYD.csv    14.94\n",
      "14        20250311_CHN.csv    11.48\n",
      "15        20250311_KOL.csv     4.76\n",
      "16        20250311_NCR.csv    41.31\n",
      "17  20250311_Bengaluru.csv    26.16\n",
      "18     20250311_Mumbai.csv    18.48\n",
      "19       20250311_Pune.csv     5.33\n",
      "20        20250310_CHN.csv    11.66\n",
      "21        20250310_KOL.csv     4.93\n",
      "22        20250310_HYD.csv    14.97\n",
      "23        20250310_NCR.csv    43.36\n",
      "24     20250310_Mumbai.csv    19.43\n",
      "25        20250309_NCR.csv    40.33\n",
      "26  20250310_Bengaluru.csv    25.01\n",
      "27       20250310_Pune.csv     7.32\n",
      "28        20250309_HYD.csv    14.55\n",
      "29        20250309_CHN.csv    12.31\n",
      "30        20250309_KOL.csv     4.51\n",
      "31     20250309_Mumbai.csv    20.42\n",
      "32       20250309_Pune.csv     6.97\n",
      "33  20250309_Bengaluru.csv    23.51 \n",
      "\n",
      "Total Data to be processed 608.37 MB\n",
      "\n",
      "File number 1 - Downloading file: 20250313_CHN.csv (Resuming from 0 bytes)\n",
      "Download 76% complete for 20250313_CHN.csv.\n",
      "Download 100% complete for 20250313_CHN.csv.\n",
      "\n",
      "File number 2 - Downloading file: 20250313_KOL.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250313_KOL.csv.\n",
      "\n",
      "File number 3 - Downloading file: 20250313_NCR.csv (Resuming from 0 bytes)\n",
      "Download 25% complete for 20250313_NCR.csv.\n",
      "Download 51% complete for 20250313_NCR.csv.\n",
      "Download 77% complete for 20250313_NCR.csv.\n",
      "Download 100% complete for 20250313_NCR.csv.\n",
      "\n",
      "File number 4 - Downloading file: 20250313_Mumbai.csv (Resuming from 0 bytes)\n",
      "Download 46% complete for 20250313_Mumbai.csv.\n",
      "Download 92% complete for 20250313_Mumbai.csv.\n",
      "Download 100% complete for 20250313_Mumbai.csv.\n",
      "\n",
      "File number 5 - Downloading file: 20250313_HYD.csv (Resuming from 0 bytes)\n",
      "Download 60% complete for 20250313_HYD.csv.\n",
      "Download 100% complete for 20250313_HYD.csv.\n",
      "\n",
      "File number 6 - Downloading file: 20250313_Pune.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250313_Pune.csv.\n",
      "\n",
      "File number 7 - Downloading file: 20250313_Bengaluru.csv (Resuming from 0 bytes)\n",
      "Download 35% complete for 20250313_Bengaluru.csv.\n",
      "Download 71% complete for 20250313_Bengaluru.csv.\n",
      "Download 100% complete for 20250313_Bengaluru.csv.\n",
      "\n",
      "File number 8 - Downloading file: 20250312_CHN.csv (Resuming from 0 bytes)\n",
      "Download 76% complete for 20250312_CHN.csv.\n",
      "Download 100% complete for 20250312_CHN.csv.\n",
      "\n",
      "File number 9 - Downloading file: 20250312_NCR.csv (Resuming from 0 bytes)\n",
      "Download 25% complete for 20250312_NCR.csv.\n",
      "Download 50% complete for 20250312_NCR.csv.\n",
      "Download 75% complete for 20250312_NCR.csv.\n",
      "Download 100% complete for 20250312_NCR.csv.\n",
      "\n",
      "File number 10 - Downloading file: 20250312_HYD.csv (Resuming from 0 bytes)\n",
      "Download 64% complete for 20250312_HYD.csv.\n",
      "Download 100% complete for 20250312_HYD.csv.\n",
      "\n",
      "File number 11 - Downloading file: 20250312_KOL.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250312_KOL.csv.\n",
      "\n",
      "File number 12 - Downloading file: 20250312_Pune.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250312_Pune.csv.\n",
      "\n",
      "File number 13 - Downloading file: 20250312_Bengaluru.csv (Resuming from 0 bytes)\n",
      "Download 35% complete for 20250312_Bengaluru.csv.\n",
      "Download 71% complete for 20250312_Bengaluru.csv.\n",
      "Download 100% complete for 20250312_Bengaluru.csv.\n",
      "\n",
      "File number 14 - Downloading file: 20250311_HYD.csv (Resuming from 0 bytes)\n",
      "Download 66% complete for 20250311_HYD.csv.\n",
      "Download 100% complete for 20250311_HYD.csv.\n",
      "\n",
      "File number 15 - Downloading file: 20250311_CHN.csv (Resuming from 0 bytes)\n",
      "Download 87% complete for 20250311_CHN.csv.\n",
      "Download 100% complete for 20250311_CHN.csv.\n",
      "\n",
      "File number 16 - Downloading file: 20250311_KOL.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250311_KOL.csv.\n",
      "\n",
      "File number 17 - Downloading file: 20250311_NCR.csv (Resuming from 0 bytes)\n",
      "Download 24% complete for 20250311_NCR.csv.\n",
      "Download 48% complete for 20250311_NCR.csv.\n",
      "Download 72% complete for 20250311_NCR.csv.\n",
      "Download 96% complete for 20250311_NCR.csv.\n",
      "Download 100% complete for 20250311_NCR.csv.\n",
      "\n",
      "File number 18 - Downloading file: 20250311_Bengaluru.csv (Resuming from 0 bytes)\n",
      "Download 38% complete for 20250311_Bengaluru.csv.\n",
      "Download 76% complete for 20250311_Bengaluru.csv.\n",
      "Download 100% complete for 20250311_Bengaluru.csv.\n",
      "\n",
      "File number 19 - Downloading file: 20250311_Mumbai.csv (Resuming from 0 bytes)\n",
      "Download 54% complete for 20250311_Mumbai.csv.\n",
      "Download 100% complete for 20250311_Mumbai.csv.\n",
      "\n",
      "File number 20 - Downloading file: 20250311_Pune.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250311_Pune.csv.\n",
      "\n",
      "File number 21 - Downloading file: 20250310_CHN.csv (Resuming from 0 bytes)\n",
      "Download 85% complete for 20250310_CHN.csv.\n",
      "Download 100% complete for 20250310_CHN.csv.\n",
      "\n",
      "File number 22 - Downloading file: 20250310_KOL.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250310_KOL.csv.\n",
      "\n",
      "File number 23 - Downloading file: 20250310_HYD.csv (Resuming from 0 bytes)\n",
      "Download 66% complete for 20250310_HYD.csv.\n",
      "Download 100% complete for 20250310_HYD.csv.\n",
      "\n",
      "File number 24 - Downloading file: 20250310_NCR.csv (Resuming from 0 bytes)\n",
      "Download 23% complete for 20250310_NCR.csv.\n",
      "Download 46% complete for 20250310_NCR.csv.\n",
      "Download 69% complete for 20250310_NCR.csv.\n",
      "Download 92% complete for 20250310_NCR.csv.\n",
      "Download 100% complete for 20250310_NCR.csv.\n",
      "\n",
      "File number 25 - Downloading file: 20250310_Mumbai.csv (Resuming from 0 bytes)\n",
      "Download 51% complete for 20250310_Mumbai.csv.\n",
      "Download 100% complete for 20250310_Mumbai.csv.\n",
      "\n",
      "File number 26 - Downloading file: 20250309_NCR.csv (Resuming from 0 bytes)\n",
      "Download 24% complete for 20250309_NCR.csv.\n",
      "Download 49% complete for 20250309_NCR.csv.\n",
      "Download 74% complete for 20250309_NCR.csv.\n",
      "Download 99% complete for 20250309_NCR.csv.\n",
      "Download 100% complete for 20250309_NCR.csv.\n",
      "\n",
      "File number 27 - Downloading file: 20250310_Bengaluru.csv (Resuming from 0 bytes)\n",
      "Error processing file 20250310_Bengaluru.csv: The read operation timed out\n",
      "\n",
      "File number 28 - Downloading file: 20250310_Pune.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250310_Pune.csv.\n",
      "\n",
      "File number 29 - Downloading file: 20250309_HYD.csv (Resuming from 0 bytes)\n",
      "Download 68% complete for 20250309_HYD.csv.\n",
      "Download 100% complete for 20250309_HYD.csv.\n",
      "\n",
      "File number 30 - Downloading file: 20250309_CHN.csv (Resuming from 0 bytes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/q9zkzrlj50sfn519p6lhnzkm0000gp/T/ipykernel_23686/2539478559.py:129: DtypeWarning: Columns (3,4,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_stream, usecols=required_columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 81% complete for 20250309_CHN.csv.\n",
      "Download 100% complete for 20250309_CHN.csv.\n",
      "\n",
      "File number 31 - Downloading file: 20250309_KOL.csv (Resuming from 0 bytes)\n",
      "Download 100% complete for 20250309_KOL.csv.\n",
      "\n",
      "File number 32 - Downloading file: 20250309_Mumbai.csv (Resuming from 0 bytes)\n",
      "Download 48% complete for 20250309_Mumbai.csv.\n",
      "Download 97% complete for 20250309_Mumbai.csv.\n",
      "Download 100% complete for 20250309_Mumbai.csv.\n",
      "\n",
      "File number 33 - Downloading file: 20250309_Pune.csv (Resuming from 0 bytes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/q9zkzrlj50sfn519p6lhnzkm0000gp/T/ipykernel_23686/2539478559.py:129: DtypeWarning: Columns (3,4,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_stream, usecols=required_columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100% complete for 20250309_Pune.csv.\n",
      "\n",
      "File number 34 - Downloading file: 20250309_Bengaluru.csv (Resuming from 0 bytes)\n",
      "Download 42% complete for 20250309_Bengaluru.csv.\n",
      "Download 85% complete for 20250309_Bengaluru.csv.\n",
      "Download 100% complete for 20250309_Bengaluru.csv.\n",
      "All files processed and merged successfully.\n",
      "Total rows before removing null values : 5299656\n",
      "Date column created successfully!\n"
     ]
    }
   ],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import pytz, os,json, requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "from googleapiclient.errors import HttpError\n",
    "import googleapiclient.http\n",
    "\n",
    "# Path to your service account key file and the scopes needed for the Drive API\n",
    "SERVICE_ACCOUNT_FILE = \"/Users/sachin/TheJuniorDataScientist/credentials/sachin_service account.json\"\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Authenticate and build the Drive API client\n",
    "def authenticate_drive_service():\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        drive_service = build('drive', 'v3', credentials=credentials)\n",
    "        print(\"Google Drive service authorized successfully.\")\n",
    "        return drive_service\n",
    "    except Exception as e:\n",
    "        print(\"Error during authentication:\", e)\n",
    "        raise\n",
    "\n",
    "# Generate date range (YYYYMMDD format)\n",
    "def get_date_range(local_tz, start_date=None, end_date=None, days=None):\n",
    "    current_time = datetime.now(pytz.utc).astimezone(local_tz)\n",
    "    if end_date:\n",
    "        end_date = datetime.strptime(end_date, \"%Y%m%d\").astimezone(local_tz)\n",
    "    else:\n",
    "        end_date = current_time\n",
    "\n",
    "    if start_date:\n",
    "        start_date = datetime.strptime(start_date, \"%Y%m%d\").astimezone(local_tz)\n",
    "    elif days:\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "    else:\n",
    "        start_date = end_date - timedelta(days=10)\n",
    "\n",
    "    return start_date.strftime(\"%Y%m%d\"), end_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "def fetch_files_with_dates(drive_service, folder_id, date_range):\n",
    "    try:\n",
    "        query = f\"'{folder_id}' in parents and mimeType='text/csv' and trashed=false\"\n",
    "        \n",
    "        files = []\n",
    "        page_token = None  # Initialize page token\n",
    "        \n",
    "        while True:\n",
    "            # Fetch a page of files\n",
    "            response = drive_service.files().list(\n",
    "                q=query,\n",
    "                fields=\"nextPageToken, files(id, name, size)\",\n",
    "                pageToken=page_token\n",
    "            ).execute()\n",
    "\n",
    "            # Extend the list with new files from this page\n",
    "            files.extend(response.get('files', []))\n",
    "\n",
    "            # Check if there's another page\n",
    "            page_token = response.get('nextPageToken')\n",
    "            if not page_token:\n",
    "                break  # No more pages, exit loop\n",
    "        \n",
    "        print(f\"Total files found in the folder: {len(files)}\")\n",
    "\n",
    "        # Filter files by date in filenames\n",
    "        filtered_files = []\n",
    "        for file in files:\n",
    "            for date in date_range:\n",
    "                if date in file['name']:\n",
    "                    file['size_mb'] = round(int(file.get('size', 0)) / (1024 * 1024), 2)  # Convert size to MB\n",
    "                    filtered_files.append(file)\n",
    "                    break\n",
    "\n",
    "        print(f\"Number of files matching the date range: {len(filtered_files)}\")\n",
    "        return filtered_files\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def process_files(drive_service, files, required_columns, checkpoint_file=\"download_progress.json\", save_interval = 50 * 1024 * 1024, chunksize=10 * 1024 * 1024):\n",
    "    combined_data = []\n",
    "    last_save_time = datetime.now()\n",
    "\n",
    "    try:\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            download_progress = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        download_progress = {}\n",
    "\n",
    "    for i, file in enumerate(files):\n",
    "        try:\n",
    "            file_id = file['id']\n",
    "            file_name = file['name']\n",
    "            file_size = int(file.get('size', 0))\n",
    "            bytes_downloaded = download_progress.get(file_name, 0)\n",
    "\n",
    "            if bytes_downloaded >= file_size:\n",
    "                print(f\"{file_name} has already been downloaded. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nFile number {i+1} - Downloading file: {file_name} (Resuming from {bytes_downloaded} bytes)\")\n",
    "\n",
    "            request = drive_service.files().get_media(fileId=file_id)\n",
    "            file_stream = BytesIO()\n",
    "            downloader = googleapiclient.http.MediaIoBaseDownload(file_stream, request, chunksize=chunksize)\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                try:\n",
    "                    status, done = downloader.next_chunk()\n",
    "                    bytes_downloaded += len(file_stream.getvalue())\n",
    "                    download_progress[file_name] = bytes_downloaded\n",
    "\n",
    "                    if (datetime.now() - last_save_time).total_seconds() > 60 or bytes_downloaded % save_interval < chunksize:\n",
    "                        with open(checkpoint_file, \"w\") as f:\n",
    "                            json.dump(download_progress, f)\n",
    "                        last_save_time = datetime.now()\n",
    "\n",
    "                    print(f\"Download {int(status.progress() * 100)}% complete for {file_name}.\")\n",
    "                except (requests.ConnectionError, HttpError) as e:\n",
    "                    print(f\"Network error during download: {e}. Retrying...\")\n",
    "                    break\n",
    "\n",
    "            file_stream.seek(0)\n",
    "            df = pd.read_csv(file_stream, usecols=required_columns)\n",
    "            df = df[required_columns]\n",
    "            df['source_file'] = file_name\n",
    "            combined_data.append(df)\n",
    "\n",
    "            if file_name in download_progress:\n",
    "                del download_progress[file_name]\n",
    "                with open(checkpoint_file, \"w\") as f:\n",
    "                    json.dump(download_progress, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    if combined_data:\n",
    "        combined_data = [df for df in combined_data if not df.empty]\n",
    "        if combined_data:\n",
    "            final_df = pd.concat(combined_data, ignore_index=True)\n",
    "            final_df[['to_qty']] = final_df[['to_qty']].fillna(0)\n",
    "            print(\"All files processed and merged successfully.\")\n",
    "            return final_df\n",
    "    else:\n",
    "        print(\"No files were successfully processed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Step 1: Authenticate Drive Service\n",
    "drive_service = authenticate_drive_service()\n",
    "\n",
    "local_tz = pytz.timezone(\"Asia/Kolkata\")\n",
    "start_date_input = None # \"20250201\"\n",
    "end_date_input =  None #\"20250225\"\n",
    "days_range = 2\n",
    "\n",
    "start_date, end_date = get_date_range(local_tz, start_date_input, end_date_input, days_range)\n",
    "date_range = pd.date_range(start=start_date, end=end_date).strftime(\"%Y%m%d\").tolist()\n",
    "print(f\"Date range for filtering: {start_date} to {end_date}\")\n",
    "\n",
    "# Step 3: Fetch Files Matching the Date Range\n",
    "folder_id = '1akFe2_iKCqoZ0lAETRipXxDSKYnFmx2x'\n",
    "filtered_files = fetch_files_with_dates(drive_service, folder_id, date_range)\n",
    "file_df = pd.DataFrame(filtered_files)\n",
    "file_df.drop(columns=['size','id'],inplace=True, axis=1)\n",
    "print(file_df, f\"\\n\\nTotal Data to be processed {file_df['size_mb'].sum()} MB\")\n",
    "\n",
    "# Step 4: Process Files and Combine Data\n",
    "required_columns = ['dd', 'mm', 'yyyy', 'mh_code', 'type', 'store_name', 'to_qty','store_id', 'product_variant_id']\n",
    "final_df = process_files(drive_service, filtered_files, required_columns)\n",
    "\n",
    "print('Total rows before removing null values :',final_df.shape[0])\n",
    "### Printing Null Records\n",
    "#final_df[final_df[['dd', 'mm', 'yyyy', 'mh_code', 'type', 'store_name', 'store_id']].isna().all(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dropping Null records\n",
    "main_df = final_df.dropna(subset=['dd', 'mm', 'yyyy', 'mh_code', 'type', 'store_name', 'store_id', 'product_variant_id'], how='all').copy()\n",
    "columns_to_convert = ['dd', 'mm', 'yyyy']\n",
    "try:\n",
    "    main_df[columns_to_convert] = main_df[columns_to_convert].fillna(1).astype(int)\n",
    "    main_df.rename(columns={'yyyy': 'year', 'mm': 'month', 'dd': 'day'}, inplace=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error during  conversion: {e}\")\n",
    "try:\n",
    "    # Creating the date column from dd, mm, yyyy\n",
    "    main_df['date'] = pd.to_datetime(main_df[['year', 'month', 'day']])\n",
    "    print(\"Date column created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during date conversion: {e}\")\n",
    "\n",
    "\n",
    "main_df = main_df.groupby(['type','mh_code', 'store_name', 'store_id', 'source_file', 'date', 'product_variant_id']).agg({'to_qty': 'sum'}).reset_index()\n",
    "main_df.rename(columns={'store_name': 'Destination_store_name', 'store_id': 'Destination_store_id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>mh_code</th>\n",
       "      <th>Destination_store_name</th>\n",
       "      <th>Destination_store_id</th>\n",
       "      <th>source_file</th>\n",
       "      <th>date</th>\n",
       "      <th>product_variant_id</th>\n",
       "      <th>to_qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COLD</td>\n",
       "      <td>KOL040M</td>\n",
       "      <td>KOL- Rickjoni</td>\n",
       "      <td>dd246eef-6cb6-4358-b455-9b46c1c444ca</td>\n",
       "      <td>20250309_KOL.csv</td>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>03ea892c-6de0-423a-a2a2-e531f220fb86</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COLD</td>\n",
       "      <td>KOL040M</td>\n",
       "      <td>KOL- Rickjoni</td>\n",
       "      <td>dd246eef-6cb6-4358-b455-9b46c1c444ca</td>\n",
       "      <td>20250309_KOL.csv</td>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>0b30de60-36d7-4789-a325-66c44d8ba9d2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COLD</td>\n",
       "      <td>KOL040M</td>\n",
       "      <td>KOL- Rickjoni</td>\n",
       "      <td>dd246eef-6cb6-4358-b455-9b46c1c444ca</td>\n",
       "      <td>20250309_KOL.csv</td>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>0cb36910-19d9-402f-99d8-37170972819e</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COLD</td>\n",
       "      <td>KOL040M</td>\n",
       "      <td>KOL- Rickjoni</td>\n",
       "      <td>dd246eef-6cb6-4358-b455-9b46c1c444ca</td>\n",
       "      <td>20250309_KOL.csv</td>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>0dc04b66-61e5-465a-954c-174270062b30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COLD</td>\n",
       "      <td>KOL040M</td>\n",
       "      <td>KOL- Rickjoni</td>\n",
       "      <td>dd246eef-6cb6-4358-b455-9b46c1c444ca</td>\n",
       "      <td>20250309_KOL.csv</td>\n",
       "      <td>2025-03-09</td>\n",
       "      <td>1363a5ff-b43b-439b-a814-a0982e8559de</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5228623</th>\n",
       "      <td>Dry</td>\n",
       "      <td>PAT002M</td>\n",
       "      <td>SAS-Zirakpur Apple Heights</td>\n",
       "      <td>7b2176ce-862d-4e58-9d5e-6373e729c2a6</td>\n",
       "      <td>20250313_NCR.csv</td>\n",
       "      <td>2025-03-13</td>\n",
       "      <td>fddeda02-caff-4a4c-96e6-580d46e5c31b</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5228624</th>\n",
       "      <td>Dry</td>\n",
       "      <td>PAT002M</td>\n",
       "      <td>SAS-Zirakpur Apple Heights</td>\n",
       "      <td>7b2176ce-862d-4e58-9d5e-6373e729c2a6</td>\n",
       "      <td>20250313_NCR.csv</td>\n",
       "      <td>2025-03-13</td>\n",
       "      <td>fe5fd19f-16e9-4772-848e-0bb472ad6797</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5228625</th>\n",
       "      <td>Dry</td>\n",
       "      <td>PAT002M</td>\n",
       "      <td>SAS-Zirakpur Apple Heights</td>\n",
       "      <td>7b2176ce-862d-4e58-9d5e-6373e729c2a6</td>\n",
       "      <td>20250313_NCR.csv</td>\n",
       "      <td>2025-03-13</td>\n",
       "      <td>fe9e0cd3-014c-4407-af64-7ca3ec180513</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5228626</th>\n",
       "      <td>Dry</td>\n",
       "      <td>PAT002M</td>\n",
       "      <td>SAS-Zirakpur Apple Heights</td>\n",
       "      <td>7b2176ce-862d-4e58-9d5e-6373e729c2a6</td>\n",
       "      <td>20250313_NCR.csv</td>\n",
       "      <td>2025-03-13</td>\n",
       "      <td>ff2f7925-64d6-4fd5-8fe8-c093de4c6488</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5228627</th>\n",
       "      <td>Dry</td>\n",
       "      <td>PAT002M</td>\n",
       "      <td>SAS-Zirakpur Apple Heights</td>\n",
       "      <td>7b2176ce-862d-4e58-9d5e-6373e729c2a6</td>\n",
       "      <td>20250313_NCR.csv</td>\n",
       "      <td>2025-03-13</td>\n",
       "      <td>fff4979b-1046-4e0a-a5b5-2b7eaf4cb0b6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5228628 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         type  mh_code      Destination_store_name  \\\n",
       "0        COLD  KOL040M               KOL- Rickjoni   \n",
       "1        COLD  KOL040M               KOL- Rickjoni   \n",
       "2        COLD  KOL040M               KOL- Rickjoni   \n",
       "3        COLD  KOL040M               KOL- Rickjoni   \n",
       "4        COLD  KOL040M               KOL- Rickjoni   \n",
       "...       ...      ...                         ...   \n",
       "5228623   Dry  PAT002M  SAS-Zirakpur Apple Heights   \n",
       "5228624   Dry  PAT002M  SAS-Zirakpur Apple Heights   \n",
       "5228625   Dry  PAT002M  SAS-Zirakpur Apple Heights   \n",
       "5228626   Dry  PAT002M  SAS-Zirakpur Apple Heights   \n",
       "5228627   Dry  PAT002M  SAS-Zirakpur Apple Heights   \n",
       "\n",
       "                         Destination_store_id       source_file       date  \\\n",
       "0        dd246eef-6cb6-4358-b455-9b46c1c444ca  20250309_KOL.csv 2025-03-09   \n",
       "1        dd246eef-6cb6-4358-b455-9b46c1c444ca  20250309_KOL.csv 2025-03-09   \n",
       "2        dd246eef-6cb6-4358-b455-9b46c1c444ca  20250309_KOL.csv 2025-03-09   \n",
       "3        dd246eef-6cb6-4358-b455-9b46c1c444ca  20250309_KOL.csv 2025-03-09   \n",
       "4        dd246eef-6cb6-4358-b455-9b46c1c444ca  20250309_KOL.csv 2025-03-09   \n",
       "...                                       ...               ...        ...   \n",
       "5228623  7b2176ce-862d-4e58-9d5e-6373e729c2a6  20250313_NCR.csv 2025-03-13   \n",
       "5228624  7b2176ce-862d-4e58-9d5e-6373e729c2a6  20250313_NCR.csv 2025-03-13   \n",
       "5228625  7b2176ce-862d-4e58-9d5e-6373e729c2a6  20250313_NCR.csv 2025-03-13   \n",
       "5228626  7b2176ce-862d-4e58-9d5e-6373e729c2a6  20250313_NCR.csv 2025-03-13   \n",
       "5228627  7b2176ce-862d-4e58-9d5e-6373e729c2a6  20250313_NCR.csv 2025-03-13   \n",
       "\n",
       "                           product_variant_id  to_qty  \n",
       "0        03ea892c-6de0-423a-a2a2-e531f220fb86     2.0  \n",
       "1        0b30de60-36d7-4789-a325-66c44d8ba9d2     1.0  \n",
       "2        0cb36910-19d9-402f-99d8-37170972819e     4.0  \n",
       "3        0dc04b66-61e5-465a-954c-174270062b30     1.0  \n",
       "4        1363a5ff-b43b-439b-a814-a0982e8559de     1.0  \n",
       "...                                       ...     ...  \n",
       "5228623  fddeda02-caff-4a4c-96e6-580d46e5c31b     2.0  \n",
       "5228624  fe5fd19f-16e9-4772-848e-0bb472ad6797     1.0  \n",
       "5228625  fe9e0cd3-014c-4407-af64-7ca3ec180513     2.0  \n",
       "5228626  ff2f7925-64d6-4fd5-8fe8-c093de4c6488     2.0  \n",
       "5228627  fff4979b-1046-4e0a-a5b5-2b7eaf4cb0b6     1.0  \n",
       "\n",
       "[5228628 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from io import BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import googleapiclient.http\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# CONFIGURATION & AUTHENTICATION\n",
    "# ===========================\n",
    "\n",
    "# Configuration\n",
    "SERVICE_ACCOUNT_FILE = \"/Users/sachin/TheJuniorDataScientist/credentials/sachin_service account.json\"\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "CHECKPOINT_FILE = \"download_progress.json\"\n",
    "CHUNKSIZE = 10 * 1024 * 1024  # 10 MB\n",
    "SAVE_INTERVAL = 50 * 1024 * 1024  # 50 MB\n",
    "\n",
    "\n",
    "# Authenticate and build the Drive API client\n",
    "def authenticate_drive_service():\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        drive_service = build('drive', 'v3', credentials=credentials)\n",
    "        print(\"✅ Google Drive service authorized successfully.\")\n",
    "        return drive_service\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error during authentication:\", e)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# DATE RANGE HANDLING\n",
    "# ===========================\n",
    "\n",
    "# Generate date range (YYYYMMDD format)\n",
    "def get_date_range(local_tz, start_date=None, end_date=None, days=None):\n",
    "    current_time = datetime.now(pytz.utc).astimezone(local_tz)\n",
    "    if end_date:\n",
    "        end_date = datetime.strptime(end_date, \"%Y%m%d\").astimezone(local_tz)\n",
    "    else:\n",
    "        end_date = current_time\n",
    "\n",
    "    if start_date:\n",
    "        start_date = datetime.strptime(start_date, \"%Y%m%d\").astimezone(local_tz)\n",
    "    elif days:\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "    else:\n",
    "        start_date = end_date - timedelta(days=10)\n",
    "\n",
    "    return start_date.strftime(\"%Y%m%d\"), end_date.strftime(\"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# FETCH FILES FROM GOOGLE DRIVE\n",
    "# ===========================\n",
    "\n",
    "def fetch_files_with_dates(drive_service, folder_id, date_range):\n",
    "    try:\n",
    "        query = f\"'{folder_id}' in parents and mimeType='text/csv' and trashed=false\"\n",
    "        files = []\n",
    "        page_token = None\n",
    "\n",
    "        while True:\n",
    "            response = drive_service.files().list(\n",
    "                q = query,\n",
    "                fields = \"nextPageToken, files(id, name, size)\",\n",
    "                pageToken = page_token\n",
    "            ).execute()\n",
    "            \n",
    "            files.extend(response.get('files', []))\n",
    "            page_token = response.get('nextPageToken')\n",
    "            if not page_token:\n",
    "                break  \n",
    "\n",
    "        print(f\"📂 Total files found in the folder: {len(files)}\")\n",
    "\n",
    "        # Filter files by date\n",
    "        filtered_files = [\n",
    "            {**file, 'size_mb': round(int(file.get('size', 0)) / (1024 * 1024), 2)} for file in files if any(date in file['name'] for date in date_range)\n",
    "        ]\n",
    "\n",
    "        print(f\"📅 Number of files matching the date range: {len(filtered_files)}\")\n",
    "        return filtered_files\n",
    "    except HttpError as error:\n",
    "        print(f\"❌ An error occurred: {error}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# FETCH FILES FROM GOOGLE DRIVE (Supports Multiple File Types)\n",
    "# ===========================\n",
    "\n",
    "def fetch_files_with_dates(drive_service, folder_id, date_range, file_types=None):\n",
    "    \"\"\"\n",
    "    Fetches files from Google Drive within the specified folder, date range, and file types.\n",
    "\n",
    "    Args:\n",
    "        drive_service (Resource): Google Drive service resource.\n",
    "        folder_id (str)         : The ID of the Google Drive folder to search.\n",
    "        date_range (list)       : List of date strings (YYYYMMDD) to filter files by their name.\n",
    "        file_types (list)       : List of file extensions to filter (e.g., ['csv', 'xlsx']).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of files that match the criteria.\n",
    "    \"\"\"\n",
    "    # Supported MIME types for different file extensions\n",
    "    mime_type_mapping = {\n",
    "        'csv': 'text/csv',\n",
    "        'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
    "        'xls': 'application/vnd.ms-excel',\n",
    "        'json': 'application/json',\n",
    "        'txt': 'text/plain',\n",
    "        'pdf': 'application/pdf',\n",
    "        'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
    "        'doc': 'application/msword',\n",
    "        'pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',\n",
    "        'ppt': 'application/vnd.ms-powerpoint',\n",
    "        'html': 'text/html',\n",
    "        'zip': 'application/zip',\n",
    "        'rar': 'application/vnd.rar',\n",
    "        'tar': 'application/x-tar',\n",
    "        'gz': 'application/gzip',\n",
    "        'jpg': 'image/jpeg',\n",
    "        'jpeg': 'image/jpeg',\n",
    "        'png': 'image/png',\n",
    "        'gif': 'image/gif',\n",
    "        'mp4': 'video/mp4',\n",
    "        'mp3': 'audio/mpeg' }\n",
    "    \n",
    "    # Create a query string for file types if specified\n",
    "    mime_queries = []\n",
    "    if file_types:\n",
    "        for file_type in file_types:\n",
    "            if file_type in mime_type_mapping:\n",
    "                mime_queries.append(f\"mimeType='{mime_type_mapping[file_type]}'\")\n",
    "            else:\n",
    "                print(f\"⚠️ Warning: Unsupported file type '{file_type}'. Skipping it.\")\n",
    "    \n",
    "    # Combine file type queries using OR if specified\n",
    "    file_type_query = f\" and ({' or '.join(mime_queries)})\" if mime_queries else \"\"\n",
    "    \n",
    "    try:\n",
    "        query = f\"'{folder_id}' in parents and trashed=false{file_type_query}\"\n",
    "        files = []\n",
    "        page_token = None\n",
    "\n",
    "        while True:\n",
    "            response = drive_service.files().list(\n",
    "                q=query,\n",
    "                fields=\"nextPageToken, files(id, name, size)\",\n",
    "                pageToken=page_token\n",
    "            ).execute()\n",
    "            \n",
    "            files.extend(response.get('files', []))\n",
    "            page_token = response.get('nextPageToken')\n",
    "            if not page_token:\n",
    "                break  \n",
    "\n",
    "        print(f\"📂 Total files found in the folder: {len(files)}\")\n",
    "\n",
    "        # Filter files by date in the filename\n",
    "        filtered_files = [\n",
    "            {**file, 'size_mb': round(int(file.get('size', 0)) / (1024 * 1024), 2)}\n",
    "            for file in files\n",
    "            if any(date in file['name'] for date in date_range)\n",
    "        ]\n",
    "\n",
    "        print(f\"📅 Number of files matching the date range: {len(filtered_files)}\")\n",
    "        return filtered_files\n",
    "    except HttpError as error:\n",
    "        print(f\"❌ An error occurred: {error}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# FILE PROCESSING & DOWNLOADING\n",
    "# ===========================\n",
    "\n",
    "def process_files(drive_service, files, required_columns):\n",
    "    combined_data = []\n",
    "    summary_report = {\"Resumed Files\": [], \"Fully Downloaded Files\": [], \"Skipped Files\": []}\n",
    "\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "            download_progress = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        download_progress = {}\n",
    "\n",
    "    for file in files:\n",
    "        file_id = file['id']\n",
    "        file_name = file['name']\n",
    "        file_size = int(file.get('size', 0))\n",
    "        bytes_downloaded = download_progress.get(file_name, 0)\n",
    "\n",
    "        if bytes_downloaded >= file_size:\n",
    "            print(f\"⏩ {file_name} already downloaded. Skipping...\")\n",
    "            summary_report[\"Skipped Files\"].append(file_name)\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n⬇️ Downloading file: {file_name} (Resuming from {bytes_downloaded} bytes)\")\n",
    "\n",
    "        request = drive_service.files().get_media(fileId=file_id)\n",
    "        file_stream = BytesIO()\n",
    "        downloader = googleapiclient.http.MediaIoBaseDownload(file_stream, request, chunksize=CHUNKSIZE)\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                status, done = downloader.next_chunk()\n",
    "                bytes_downloaded += len(file_stream.getvalue())\n",
    "                download_progress[file_name] = bytes_downloaded\n",
    "\n",
    "                # Save progress\n",
    "                with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "                    json.dump(download_progress, f)\n",
    "\n",
    "                print(f\"✅ Download {int(status.progress() * 100)}% complete for {file_name}.\")\n",
    "                \n",
    "                if done:\n",
    "                    file_stream.seek(0)\n",
    "                    df = pd.read_csv(file_stream, usecols=required_columns)\n",
    "                    df['source_file'] = file_name\n",
    "                    combined_data.append(df)\n",
    "                    summary_report[\"Fully Downloaded Files\"].append(file_name)\n",
    "                    del download_progress[file_name]\n",
    "                    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "                        json.dump(download_progress, f)\n",
    "                    break\n",
    "        except (requests.ConnectionError, HttpError) as e:\n",
    "            print(f\"❌ Network error during download: {e}. Saving progress...\")\n",
    "            summary_report[\"Resumed Files\"].append(file_name)\n",
    "\n",
    "    if combined_data:\n",
    "        final_df = pd.concat(combined_data, ignore_index=True)\n",
    "        final_df[['to_qty']] = final_df[['to_qty']].fillna(0)\n",
    "        return final_df, summary_report\n",
    "    else:\n",
    "        return pd.DataFrame(), summary_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google Drive service authorized successfully.\n",
      "📂 Total files found in the folder: 4142\n",
      "📅 Number of files matching the date range: 13\n",
      "\n",
      "⬇️ Downloading file: 20250313_CHN.csv (Resuming from 0 bytes)\n",
      "✅ Download 76% complete for 20250313_CHN.csv.\n",
      "✅ Download 100% complete for 20250313_CHN.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250313_KOL.csv (Resuming from 0 bytes)\n",
      "✅ Download 100% complete for 20250313_KOL.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250313_NCR.csv (Resuming from 0 bytes)\n",
      "✅ Download 25% complete for 20250313_NCR.csv.\n",
      "✅ Download 51% complete for 20250313_NCR.csv.\n",
      "✅ Download 77% complete for 20250313_NCR.csv.\n",
      "✅ Download 100% complete for 20250313_NCR.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250313_Mumbai.csv (Resuming from 0 bytes)\n",
      "✅ Download 46% complete for 20250313_Mumbai.csv.\n",
      "✅ Download 92% complete for 20250313_Mumbai.csv.\n",
      "✅ Download 100% complete for 20250313_Mumbai.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250313_HYD.csv (Resuming from 0 bytes)\n",
      "✅ Download 60% complete for 20250313_HYD.csv.\n",
      "✅ Download 100% complete for 20250313_HYD.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250313_Pune.csv (Resuming from 0 bytes)\n",
      "✅ Download 100% complete for 20250313_Pune.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250313_Bengaluru.csv (Resuming from 0 bytes)\n",
      "✅ Download 35% complete for 20250313_Bengaluru.csv.\n",
      "✅ Download 71% complete for 20250313_Bengaluru.csv.\n",
      "✅ Download 100% complete for 20250313_Bengaluru.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250312_CHN.csv (Resuming from 0 bytes)\n",
      "✅ Download 76% complete for 20250312_CHN.csv.\n",
      "✅ Download 100% complete for 20250312_CHN.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250312_NCR.csv (Resuming from 0 bytes)\n",
      "✅ Download 25% complete for 20250312_NCR.csv.\n",
      "✅ Download 50% complete for 20250312_NCR.csv.\n",
      "✅ Download 75% complete for 20250312_NCR.csv.\n",
      "✅ Download 100% complete for 20250312_NCR.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250312_HYD.csv (Resuming from 0 bytes)\n",
      "✅ Download 64% complete for 20250312_HYD.csv.\n",
      "✅ Download 100% complete for 20250312_HYD.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250312_KOL.csv (Resuming from 0 bytes)\n",
      "✅ Download 100% complete for 20250312_KOL.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250312_Pune.csv (Resuming from 0 bytes)\n",
      "✅ Download 100% complete for 20250312_Pune.csv.\n",
      "\n",
      "⬇️ Downloading file: 20250312_Bengaluru.csv (Resuming from 0 bytes)\n",
      "✅ Download 35% complete for 20250312_Bengaluru.csv.\n",
      "✅ Download 71% complete for 20250312_Bengaluru.csv.\n",
      "✅ Download 100% complete for 20250312_Bengaluru.csv.\n",
      "\n",
      "📌 Summary Report:\n",
      "Resumed Files: 0 files\n",
      "Fully Downloaded Files: 13 files\n",
      "Skipped Files: 0 files\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# MAIN EXECUTION FLOW\n",
    "# ===========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    drive_service = authenticate_drive_service()\n",
    "\n",
    "    local_tz = pytz.timezone(\"Asia/Kolkata\")\n",
    "    start_date, end_date = get_date_range(local_tz, days=2)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date).strftime(\"%Y%m%d\").tolist()\n",
    "\n",
    "    folder_id = '1akFe2_iKCqoZ0lAETRipXxDSKYnFmx2x'\n",
    "    filtered_files = fetch_files_with_dates(drive_service, folder_id, date_range)\n",
    "\n",
    "    required_columns = ['dd', 'mm', 'yyyy', 'mh_code', 'type', 'store_name', 'to_qty', 'store_id', 'product_variant_id']\n",
    "    final_df, summary_report = process_files(drive_service, filtered_files, required_columns)\n",
    "\n",
    "    print(\"\\n📌 Summary Report:\")\n",
    "    for category, files in summary_report.items():\n",
    "        print(f\"{category}: {len(files)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
